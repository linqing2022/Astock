{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:30:59.699352Z",
     "start_time": "2022-02-08T10:30:55.805594Z"
    },
    "id": "dPdpNulr5rdc"
   },
   "outputs": [],
   "source": [
    "%reload_ext watermark\n",
    "%watermark -v -p numpy,pandas,torch,transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:01.038190Z",
     "start_time": "2022-02-08T10:30:59.700337Z"
    },
    "id": "pEHKZt-qRG5x"
   },
   "outputs": [],
   "source": [
    "#@title Setup & Config\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import defaultdict\n",
    "from textwrap import wrap\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from collections import Counter\n",
    "import ast\n",
    "import random\n",
    "import json\n",
    "from math import isnan\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" ###指定此处为-1即可\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)\n",
    "\n",
    "HAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n",
    "\n",
    "sns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n",
    "\n",
    "rcParams['figure.figsize'] = 12, 8\n",
    "\n",
    "RANDOM_SEED = 10\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHAlx_DgsuY6"
   },
   "source": [
    "# New Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:05.994993Z",
     "start_time": "2022-02-08T10:31:01.041641Z"
    },
    "id": "xRdWcGGBFYTu"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer,BertModel\n",
    "# PRE_TRAINED_MODEL_NAME = \"../hfl/chinese-roberta-wwm-ext\"\n",
    "# PRE_TRAINED_MODEL_NAME = \"hfl/chinese-roberta-wwm-ext-large\"\n",
    "# PRE_TRAINED_MODEL_NAME = \"hfl/chinese-bert-wwm-ext\"\n",
    "# PRE_TRAINED_MODEL_NAME = \"bert-base-uncased\"\n",
    "# PRE_TRAINED_MODEL_NAME = \"ProsusAI/finbert\"\n",
    "PRE_TRAINED_MODEL_NAME = \"../hfl/bert-base-chinese\"\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME,do_lower_case=True)\n",
    "PRE_TRAINED_MODEL_NAME = '../model/ROBERT_4_model.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:05.999077Z",
     "start_time": "2022-02-08T10:31:05.995761Z"
    },
    "id": "EW_JmYxCNyLa"
   },
   "outputs": [],
   "source": [
    "class GPReviewDataset(Dataset):\n",
    "\n",
    "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
    "    self.reviews = reviews\n",
    "    self.targets = targets\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.reviews)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    review = str(self.reviews[item])\n",
    "    target = self.targets[item]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      review,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding='max_length',\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'review_text': review,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:06.548011Z",
     "start_time": "2022-02-08T10:31:05.999678Z"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "df_train = pd.read_csv('d:/data/train.csv',sep='\\t')\n",
    "df_val = pd.read_csv('d:/data/val.csv',sep='\\t')\n",
    "df_test = pd.read_csv('d:/data/test.csv',sep='\\t')\n",
    "df_ood = pd.read_csv('d:/data/ood.csv',sep='\\t')\n",
    "\n",
    "\n",
    "df_train = df_train.drop(df_train.loc[df_train.verbA0A1.isna()].index)\n",
    "df_test = df_test.drop(df_test.loc[df_test.verbA0A1.isna()].index)\n",
    "df_val = df_val.drop(df_val.loc[df_val.verbA0A1.isna()].index)\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1.isna()].index)\n",
    "\n",
    "df_train = df_train.drop(df_train.loc[df_train.verbA0A1=='[]'].index)\n",
    "df_test = df_test.drop(df_test.loc[df_test.verbA0A1=='[]'].index)\n",
    "df_val = df_val.drop(df_val.loc[df_val.verbA0A1=='[]'].index)\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1=='[]'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:32.569116Z",
     "start_time": "2022-02-08T10:31:06.548859Z"
    },
    "id": "-zD37Z7TN_ii"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def string_to_tuples_list(text):\n",
    "  if text is np.nan or text =='[]':\n",
    "    return []\n",
    "  text = ''.join(text.split('], ['))\n",
    "  tmp = eval(text.strip('[').strip(']'))\n",
    "  if not isinstance(tmp[0],tuple):\n",
    "    return [tmp]\n",
    "  return list(tmp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for col in ['verb','A0','A1']:\n",
    "  df_train[col] = df_train[col].apply(string_to_tuples_list)\n",
    "  df_val[col] = df_val[col].apply(string_to_tuples_list)\n",
    "  df_test[col] = df_test[col].apply(string_to_tuples_list)\n",
    "  df_ood[col] = df_ood[col].apply(string_to_tuples_list)\n",
    "\n",
    "for col in ['stock_factors','verbA0A1']:\n",
    "# for col in ['verbA0A1']:\n",
    "  df_train[col] = df_train[col].apply(ast.literal_eval)\n",
    "  df_val[col] = df_val[col].apply(ast.literal_eval)\n",
    "  df_test[col] = df_test[col].apply(ast.literal_eval)\n",
    "  df_ood[col] = df_ood[col].apply(ast.literal_eval)\n",
    "\n",
    "\n",
    "\n",
    "def mask(df):\n",
    "  df = df.reset_index(drop = True)\n",
    "  df['verb_mask'] = 0\n",
    "  df['A0_mask'] = 0\n",
    "  df['A1_mask'] = 0\n",
    "  df['verb_mask'] = df['verb_mask'].astype('object')\n",
    "  df['A0_mask'] = df['A0_mask'].astype('object')\n",
    "  df['A1_mask'] = df['A1_mask'].astype('object')\n",
    "  for index,row in df.iterrows():\n",
    "\n",
    "    df.at[index,'stock_factors'] = [*map(float,df.loc[index,'stock_factors'])]\n",
    "    AV_num = 0\n",
    "    for k,col in enumerate(['verb','A0','A1']):\n",
    "      masks = []\n",
    "      for j in range(len(row['verbA0A1'])):\n",
    "        mask = np.zeros(299)\n",
    "        idx = []\n",
    "        for v in row['verbA0A1'][j][k]:\n",
    "          \n",
    "          idx = idx + [int(i) for i in range(v[0],v[0]+v[1])]\n",
    "        # idx = np.unique(idx).tolist()\n",
    "        counter = Counter(idx)\n",
    "\n",
    "        mask = [0 if counter[i]== 0 else 1/len(counter) for i in range(0,len(mask))]\n",
    "        mask.insert(0,0)\n",
    "        masks.append(mask)\n",
    "      AV_num = len(masks)\n",
    "      for i in range(10 - len(masks)):\n",
    "        masks.append(np.zeros(300))\n",
    "      while len(masks)>10:\n",
    "        masks.pop()\n",
    "      name = col+'_mask'\n",
    "      df.at[index,name] = np.array(masks)\n",
    "    if AV_num>10:\n",
    "      AV_num=10\n",
    "    df.loc[index,'AV_num'] = int(AV_num)\n",
    "  df.AV_num = df.AV_num.astype('int')\n",
    "#   df.stock_factors = df.stock_factors.apply(np.array)\n",
    "  return df\n",
    "\n",
    "\n",
    "df_train = mask(df_train)\n",
    "df_test = mask(df_test)\n",
    "df_val = mask(df_val)\n",
    "df_ood = mask(df_ood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:32.574892Z",
     "start_time": "2022-02-08T10:31:32.570081Z"
    }
   },
   "outputs": [],
   "source": [
    "max_len = 300\n",
    "class_names = ['negative','neutral', 'positive']\n",
    "class GPReviewDataset(Dataset):\n",
    "\n",
    "  def __init__(self, reviews, targets,verb,A0,A1,AV_num,tokenizer,stock_factors, max_len):\n",
    "    self.reviews = reviews\n",
    "    self.targets = targets\n",
    "    self.stock_factors = stock_factors\n",
    "    self.verb = verb\n",
    "    self.A0 = A0\n",
    "    self.A1 = A1\n",
    "    self.AV_num = AV_num\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.reviews)\n",
    "  \n",
    "  def __getitem__(self, item):\n",
    "    review = str(self.reviews[item])\n",
    "    target = self.targets[item]\n",
    "    stock_factors = self.stock_factors[item]\n",
    "    v = self.verb[item]\n",
    "    a0 = self.A0[item]\n",
    "    a1 = self.A1[item]\n",
    "    av_num = self.AV_num[item]\n",
    "\n",
    "    encoding = self.tokenizer.encode_plus(\n",
    "      review,\n",
    "      add_special_tokens=True,\n",
    "      max_length=self.max_len,\n",
    "      return_token_type_ids=False,\n",
    "      padding='max_length',\n",
    "      truncation=True,\n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt',\n",
    "    )\n",
    "\n",
    "    return {\n",
    "      'review_text': review,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(target, dtype=torch.long),\n",
    "      'stock_factors':torch.tensor(stock_factors),\n",
    "      'verb': torch.tensor(v),\n",
    "      'A0': torch.tensor(a0),\n",
    "      'A1': torch.tensor(a1),\n",
    "      'AV_num': torch.tensor(av_num)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:32.588806Z",
     "start_time": "2022-02-08T10:31:32.575686Z"
    },
    "id": "cPZSIigpPtEr"
   },
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = GPReviewDataset(\n",
    "    reviews=df.text_a.to_numpy(),\n",
    "    targets=df.label.to_numpy(),\n",
    "    stock_factors = df.stock_factors,\n",
    "    verb = df.verb_mask,\n",
    "    A0 = df.A0_mask,\n",
    "    A1 = df.A1_mask,\n",
    "    AV_num = df.AV_num,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:32.610052Z",
     "start_time": "2022-02-08T10:31:32.595401Z"
    },
    "id": "tm2qZ9OofxC9"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "train_data_loader = create_data_loader(df_train, tokenizer, max_len, BATCH_SIZE)\n",
    "val_data_loader = create_data_loader(df_val, tokenizer, max_len, BATCH_SIZE)\n",
    "test_data_loader = create_data_loader(df_test, tokenizer, max_len, BATCH_SIZE)\n",
    "ood_data_loader = create_data_loader(df_ood, tokenizer, max_len, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bidtjT6UEKkk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:32.614790Z",
     "start_time": "2022-02-08T10:31:32.610966Z"
    },
    "id": "hsRtLnPlWulS"
   },
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "\n",
    "  def __init__(self, n_classes):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME,output_attentions=True)\n",
    "    self.drop = nn.Dropout(p=0.1)\n",
    "    self.sig = nn.Sigmoid()\n",
    "    # self.relu = nn.ReLU()\n",
    "    # self.L1 = nn.Linear(self.bert.config.hidden_size,self.bert.config.hidden_size//2)\n",
    "    self.out1 = nn.Linear(self.bert.config.hidden_size, self.bert.config.hidden_size)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  \n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )[1]\n",
    "\n",
    "#     print(result.keys())\n",
    "\n",
    "#     attention_weight = torch.sum(result.attentions[11][:,:,:,:],1)[:,0,:]\n",
    "#     print(pooled_output.keys())\n",
    "\n",
    "#     output = torch.mean(pooled_output,1)\n",
    "#     output = self.drop(pooled_output)\n",
    "    # # output = self.L1(output)\n",
    "#     output = self.out(output)\n",
    "#     output = self.flatten(transformer_output.float())\n",
    "    output = self.sig(pooled_output)\n",
    "#     output = self.drop(output)\n",
    "    output = self.out1(output)\n",
    "    output = self.sig(output)\n",
    "    output = self.drop(output)\n",
    "    output = self.out(output)\n",
    " \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:35.548891Z",
     "start_time": "2022-02-08T10:31:32.616542Z"
    },
    "id": "KQEOV2pfYBPs"
   },
   "outputs": [],
   "source": [
    "model = SentimentClassifier(3)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:35.570851Z",
     "start_time": "2022-02-08T10:31:35.549862Z"
    },
    "id": "IfJO5a0NGPf_"
   },
   "outputs": [],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:35.574414Z",
     "start_time": "2022-02-08T10:31:35.571853Z"
    },
    "id": "uLl-umJKYGtB",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_ids = data['input_ids'].to(device)\n",
    "attention_mask = data['attention_mask'].to(device)\n",
    "\n",
    "print(input_ids.shape) # batch size x seq length\n",
    "print(attention_mask.shape) # batch size x seq length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:36.016876Z",
     "start_time": "2022-02-08T10:31:35.575101Z"
    }
   },
   "outputs": [],
   "source": [
    "output = model(input_ids, attention_mask)\n",
    "\n",
    "# output.attentions[0][0].shape\n",
    "\n",
    "# len(output.attentions)\n",
    "\n",
    "# output.attentions[0][:,:,:,:]\n",
    "\n",
    "# torch.sum(output.attentions[0][:,:,:,:],1)[:,1,:]\n",
    "\n",
    "# output.attentions[0]\n",
    "\n",
    "# F.softmax(output[0], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:36.024088Z",
     "start_time": "2022-02-08T10:31:36.017752Z"
    },
    "id": "4q2fUr9TYMxW"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "  optimizer,\n",
    "  num_warmup_steps=0,\n",
    "  num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:36.030991Z",
     "start_time": "2022-02-08T10:31:36.025271Z"
    },
    "id": "oLLFWDvDYeeS"
   },
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "  model, \n",
    "  data_loader, \n",
    "  loss_fn, \n",
    "  optimizer, \n",
    "  device, \n",
    "  scheduler, \n",
    "  n_examples\n",
    "):\n",
    "  model = model.train()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "  # print(len(data_loader))\n",
    "  # i = 0\n",
    "  for d in tqdm(data_loader):\n",
    "    input_ids = d[\"input_ids\"].to(device)\n",
    "    attention_mask = d[\"attention_mask\"].to(device)\n",
    "    targets = d[\"targets\"].to(device)\n",
    "    # print(i,datetime.datetime.now())\n",
    "    # i += 1\n",
    "    outputs = model(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    loss = loss_fn(outputs, targets)\n",
    "\n",
    "    correct_predictions += torch.sum(preds == targets)\n",
    "    losses.append(loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:36.036544Z",
     "start_time": "2022-02-08T10:31:36.032036Z"
    },
    "id": "VmDYxiXDYl-u"
   },
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "  model = model.eval()\n",
    "\n",
    "  losses = []\n",
    "  correct_predictions = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      loss = loss_fn(outputs, targets)\n",
    "\n",
    "      correct_predictions += torch.sum(preds == targets)\n",
    "      losses.append(loss.item())\n",
    "\n",
    "  return correct_predictions.double() / n_examples, np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:31:36.041834Z",
     "start_time": "2022-02-08T10:31:36.037208Z"
    },
    "id": "Nkvo-2p93Dnr"
   },
   "outputs": [],
   "source": [
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "# EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:53:38.357202Z",
     "start_time": "2022-02-08T10:31:36.042799Z"
    },
    "id": "VZvKK2TkYojZ",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
    "  print('-' * 10)\n",
    "\n",
    "  train_acc, train_loss = train_epoch(\n",
    "    model,\n",
    "    train_data_loader,    \n",
    "    loss_fn, \n",
    "    optimizer, \n",
    "    device, \n",
    "    scheduler, \n",
    "    len(df_train)\n",
    "  )\n",
    "\n",
    "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
    "\n",
    "  val_acc, val_loss = eval_model(\n",
    "    model,\n",
    "    val_data_loader,\n",
    "    loss_fn, \n",
    "    device, \n",
    "    len(df_val)\n",
    "  )\n",
    "\n",
    "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
    "  print()\n",
    "\n",
    "  history['train_acc'].append(train_acc)\n",
    "  history['train_loss'].append(train_loss)\n",
    "  history['val_acc'].append(val_acc)\n",
    "  history['val_loss'].append(val_loss)\n",
    "\n",
    "  if val_acc > best_accuracy:\n",
    "    torch.save(model.state_dict(), 'Pretrained_RoBERT.bin')\n",
    "    best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:53:38.360820Z",
     "start_time": "2022-02-08T10:53:38.358158Z"
    },
    "id": "FIyLcNepYr4f"
   },
   "outputs": [],
   "source": [
    "len(history[\"train_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:53:38.507047Z",
     "start_time": "2022-02-08T10:53:38.361525Z"
    },
    "id": "xu7Y2BPzLZgi"
   },
   "outputs": [],
   "source": [
    "plt.plot([i.cpu() for i in history['train_acc']], label='train accuracy')\n",
    "plt.plot([i.cpu() for i in history['val_acc']], label='validation accuracy')\n",
    "\n",
    "plt.title('Training history')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "plt.ylim([0, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:53:38.509825Z",
     "start_time": "2022-02-08T10:53:38.508040Z"
    },
    "id": "mwwZzIqBfNvG"
   },
   "outputs": [],
   "source": [
    "class_names={'negative','neutral','positive'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:53:39.268017Z",
     "start_time": "2022-02-08T10:53:38.510561Z"
    },
    "id": "YYMhWiIrBrV4"
   },
   "outputs": [],
   "source": [
    "# !gdown --id 1V8itWtowCYnb2Bc9KlK9SxGff9WwmogA\n",
    "\n",
    "model = SentimentClassifier(len(class_names))\n",
    "model.load_state_dict(torch.load('Pretrained_RoBERT.bin'))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:53:39.271069Z",
     "start_time": "2022-02-08T10:53:39.268960Z"
    },
    "id": "KYRPY0C-fFQh"
   },
   "outputs": [],
   "source": [
    "len(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T10:53:44.712567Z",
     "start_time": "2022-02-08T10:53:39.271757Z"
    },
    "id": "c7Cckbz8GbDK"
   },
   "outputs": [],
   "source": [
    "test_acc, _ = eval_model(\n",
    "  model,\n",
    "  test_data_loader,\n",
    "  loss_fn,\n",
    "  device,\n",
    "  len(df_test)\n",
    ")\n",
    "\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:55.246232Z",
     "start_time": "2022-02-08T08:44:55.241813Z"
    },
    "id": "aKwdpn1PGfIy"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "  model = model.eval()\n",
    "  \n",
    "  review_texts = []\n",
    "  predictions = []\n",
    "  prediction_probs = []\n",
    "  real_values = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for d in data_loader:\n",
    "\n",
    "      texts = d[\"review_text\"]\n",
    "      input_ids = d[\"input_ids\"].to(device)\n",
    "      attention_mask = d[\"attention_mask\"].to(device)\n",
    "      targets = d[\"targets\"].to(device)\n",
    "\n",
    "      outputs = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "      )\n",
    "      _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "      probs = F.softmax(outputs, dim=1)\n",
    "\n",
    "      review_texts.extend(texts)\n",
    "      predictions.extend(preds)\n",
    "      prediction_probs.extend(probs)\n",
    "      real_values.extend(targets)\n",
    "\n",
    "  predictions = torch.stack(predictions).cpu()\n",
    "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
    "  real_values = torch.stack(real_values).cpu()\n",
    "  return review_texts, predictions, prediction_probs, real_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:58.094492Z",
     "start_time": "2022-02-08T08:44:58.092104Z"
    },
    "id": "UIwgaRKNDPcS"
   },
   "outputs": [],
   "source": [
    "def show_confusion_matrix(confusion_matrix):\n",
    "  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
    "  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
    "  plt.ylabel('True sentiment')\n",
    "  plt.xlabel('Predicted sentiment');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:45:00.972382Z",
     "start_time": "2022-02-08T08:45:00.941048Z"
    },
    "id": "YkPHXF0MGiCh",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
    "  model,\n",
    "  test_data_loader\n",
    ")\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=class_names,digits = 4))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "df_cm = pd.DataFrame(cm, index=list(class_names), columns=list(class_names))\n",
    "show_confusion_matrix(df_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:27.552144Z",
     "start_time": "2022-02-08T08:44:27.552138Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_ood_review_texts, y_ood_pred, y_ood_pred_probs, y_ood = get_predictions(\n",
    "  model,\n",
    "  ood_data_loader\n",
    ")\n",
    "\n",
    "print(classification_report(y_ood, y_ood_pred, target_names=class_names,digits=4))\n",
    "\n",
    "ood_cm = confusion_matrix(y_ood, y_ood_pred)\n",
    "df_ood_cm = pd.DataFrame(ood_cm, index=list(class_names), columns=list(class_names))\n",
    "show_confusion_matrix(df_ood_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:27.552650Z",
     "start_time": "2022-02-08T08:44:27.552644Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ood = pd.read_csv('d:\\\\data\\\\ood.csv',sep='\\t')\n",
    "df_ood = df_ood.loc[(df_ood.DATE>='2021-01-01')&(df_ood.DATE<='2021-03-31')]\n",
    "df_ood = df_ood.sort_values(by='DATE')\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "# df_ood = df_ood.loc[:4000]\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1.isna()].index)\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1=='[]'].index)\n",
    "for col in ['verb','A0','A1']:\n",
    "  df_ood[col] = df_ood[col].apply(string_to_tuples_list)\n",
    "\n",
    "for col in ['stock_factors','verbA0A1']:\n",
    "  df_ood[col] = df_ood[col].apply(ast.literal_eval)\n",
    "df_ood = mask(df_ood)\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "ood_data_loader = create_data_loader(df_ood, tokenizer, max_len, BATCH_SIZE)\n",
    "\n",
    "y_ood_review_texts, y_ood_pred, y_ood_pred_probs, y_ood = get_predictions(\n",
    "  model,\n",
    "  ood_data_loader\n",
    ")\n",
    "\n",
    "print(classification_report(y_ood, y_ood_pred, target_names=class_names,digits=4))\n",
    "\n",
    "ood_cm = confusion_matrix(y_ood, y_ood_pred)\n",
    "df_ood_cm = pd.DataFrame(ood_cm, index=list(class_names), columns=list(class_names))\n",
    "show_confusion_matrix(df_ood_cm)\n",
    "plt.show()\n",
    "\n",
    "df_ood = pd.read_csv('d:\\\\data\\\\ood.csv',sep='\\t')\n",
    "df_ood = df_ood.loc[(df_ood.DATE>='2021-04-01')&(df_ood.DATE<='2021-06-31')]\n",
    "df_ood = df_ood.sort_values(by='DATE')\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "# df_ood = df_ood.loc[:4000]\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1.isna()].index)\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1=='[]'].index)\n",
    "for col in ['verb','A0','A1']:\n",
    "  df_ood[col] = df_ood[col].apply(string_to_tuples_list)\n",
    "\n",
    "for col in ['stock_factors','verbA0A1']:\n",
    "  df_ood[col] = df_ood[col].apply(ast.literal_eval)\n",
    "df_ood = mask(df_ood)\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "ood_data_loader = create_data_loader(df_ood, tokenizer, max_len, BATCH_SIZE)\n",
    "\n",
    "y_ood_review_texts, y_ood_pred, y_ood_pred_probs, y_ood = get_predictions(\n",
    "  model,\n",
    "  ood_data_loader\n",
    ")\n",
    "\n",
    "print(classification_report(y_ood, y_ood_pred, target_names=class_names,digits=4))\n",
    "\n",
    "ood_cm = confusion_matrix(y_ood, y_ood_pred)\n",
    "df_ood_cm = pd.DataFrame(ood_cm, index=list(class_names), columns=list(class_names))\n",
    "show_confusion_matrix(df_ood_cm)\n",
    "plt.show()\n",
    "\n",
    "df_ood = pd.read_csv('d:\\\\data\\\\ood.csv',sep='\\t')\n",
    "df_ood = df_ood.loc[(df_ood.DATE>='2021-07-01')&(df_ood.DATE<='2021-09-31')]\n",
    "df_ood = df_ood.sort_values(by='DATE')\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "# df_ood = df_ood.loc[:4000]\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1.isna()].index)\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1=='[]'].index)\n",
    "for col in ['verb','A0','A1']:\n",
    "  df_ood[col] = df_ood[col].apply(string_to_tuples_list)\n",
    "\n",
    "for col in ['stock_factors','verbA0A1']:\n",
    "  df_ood[col] = df_ood[col].apply(ast.literal_eval)\n",
    "df_ood = mask(df_ood)\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "ood_data_loader = create_data_loader(df_ood, tokenizer, max_len, BATCH_SIZE)\n",
    "\n",
    "y_ood_review_texts, y_ood_pred, y_ood_pred_probs, y_ood = get_predictions(\n",
    "  model,\n",
    "  ood_data_loader\n",
    ")\n",
    "\n",
    "print(classification_report(y_ood, y_ood_pred, target_names=class_names,digits=4))\n",
    "\n",
    "ood_cm = confusion_matrix(y_ood, y_ood_pred)\n",
    "df_ood_cm = pd.DataFrame(ood_cm, index=list(class_names), columns=list(class_names))\n",
    "show_confusion_matrix(df_ood_cm)\n",
    "plt.show()\n",
    "\n",
    "df_ood = pd.read_csv('d:\\\\data\\\\ood.csv',sep='\\t')\n",
    "df_ood = df_ood.loc[df_ood.DATE>='2021-10-01']\n",
    "df_ood = df_ood.sort_values(by='DATE')\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "# df_ood = df_ood.loc[:4000]\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1.isna()].index)\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1=='[]'].index)\n",
    "for col in ['verb','A0','A1']:\n",
    "  df_ood[col] = df_ood[col].apply(string_to_tuples_list)\n",
    "\n",
    "for col in ['stock_factors','verbA0A1']:\n",
    "  df_ood[col] = df_ood[col].apply(ast.literal_eval)\n",
    "df_ood = mask(df_ood)\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "ood_data_loader = create_data_loader(df_ood, tokenizer, max_len, BATCH_SIZE)\n",
    "\n",
    "y_ood_review_texts, y_ood_pred, y_ood_pred_probs, y_ood = get_predictions(\n",
    "  model,\n",
    "  ood_data_loader\n",
    ")\n",
    "\n",
    "print(classification_report(y_ood, y_ood_pred, target_names=class_names,digits=4))\n",
    "\n",
    "ood_cm = confusion_matrix(y_ood, y_ood_pred)\n",
    "df_ood_cm = pd.DataFrame(ood_cm, index=list(class_names), columns=list(class_names))\n",
    "show_confusion_matrix(df_ood_cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:27.553284Z",
     "start_time": "2022-02-08T08:44:27.553278Z"
    }
   },
   "outputs": [],
   "source": [
    "df_ood = pd.read_csv('d:\\\\data\\\\df_all_year_srl.csv',sep='\\t')\n",
    "# df_ood = df_ood.loc[(df_ood.DATE>='2021-05-05')&(df_ood.DATE<='2021-09-01')]\n",
    "df_ood = df_ood.sort_values(by='DATE')\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "# df_ood = df_ood.loc[:4000]\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1.isna()].index)\n",
    "df_ood = df_ood.drop(df_ood.loc[df_ood.verbA0A1=='[]'].index)\n",
    "for col in ['verb','A0','A1']:\n",
    "  df_ood[col] = df_ood[col].apply(string_to_tuples_list)\n",
    "\n",
    "for col in ['stock_factors','verbA0A1']:\n",
    "  df_ood[col] = df_ood[col].apply(ast.literal_eval)\n",
    "df_ood = mask(df_ood)\n",
    "df_ood = df_ood.reset_index(drop=True)\n",
    "ood_data_loader = create_data_loader(df_ood, tokenizer, max_len, BATCH_SIZE)\n",
    "\n",
    "y_ood_review_texts, y_ood_pred, y_ood_pred_probs, y_ood = get_predictions(\n",
    "  model,\n",
    "  ood_data_loader\n",
    ")\n",
    "\n",
    "print(classification_report(y_ood, y_ood_pred, target_names=class_names,digits=4))\n",
    "\n",
    "ood_cm = confusion_matrix(y_ood, y_ood_pred)\n",
    "df_ood_cm = pd.DataFrame(ood_cm, index=list(class_names), columns=list(class_names))\n",
    "show_confusion_matrix(df_ood_cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-08T08:44:27.553858Z",
     "start_time": "2022-02-08T08:44:27.553852Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame([y_ood_review_texts, y_ood_pred.numpy(), y_ood_pred_probs.numpy(), y_ood.numpy()]).T\n",
    "df = df.rename(columns={0:'text',1:'prediction',2:'probability',3:'labels'})\n",
    "df.to_csv('Pretrained_RoBERT_ood.csv',sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Pretrained_RoBERT.ipynb",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "17ul_8r9Y_9ICAGtq-Wv-F4BtTFqck-ul",
     "timestamp": 1634181641588
    },
    {
     "file_id": "1bn3tYjWp7f5Uc_K5Qwkm9Z-Hrp9TeO0T",
     "timestamp": 1633307974918
    }
   ]
  },
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e5e0f14df34ce411df7823f66bf6ac80b4684b7a39d44f4bdf2b22f0d1295c2a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
